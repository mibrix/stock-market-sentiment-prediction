{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "__wBAQk9AZ30"
      },
      "outputs": [],
      "source": [
        "# Standard Library Imports\n",
        "import json\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from itertools import product\n",
        "from pickle import dump\n",
        "\n",
        "# Third-Party Library Imports\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ta\n",
        "import yfinance as yf\n",
        "import xgboost as xgb\n",
        "\n",
        "# Scikit-learn Imports\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    f1_score,\n",
        "    mean_absolute_error,\n",
        "    mean_squared_error,\n",
        ")\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# TensorFlow/Keras Imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Bidirectional, Dense, Dropout, LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNGLW2LcOaxe"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dTyl3-Fsy4h5"
      },
      "outputs": [],
      "source": [
        "def df_for_inp(data, prices, ticker: str, include_sentiment: bool = True) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Prepares a DataFrame of stock prices optionally merged with Twitter sentiment data for a specific stock ticker.\n",
        "\n",
        "    Parameters:\n",
        "        data (pd.DataFrame): Twitter sentiment data with 'Stock Name' and 'Day' columns (optional if sentiment is excluded).\n",
        "        prices (pd.DataFrame): Stock price data with 'Stock Name' and 'Date' columns.\n",
        "        ticker (str): The stock ticker to filter for.\n",
        "        include_sentiment (bool): If True, includes sentiment data in the final DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A cleaned DataFrame ready for modeling or analysis.\n",
        "    \"\"\"\n",
        "    # Filter and preprocess price data\n",
        "    price_df = prices[prices['Stock Name'] == ticker].copy()\n",
        "    price_df['Date'] = pd.to_datetime(price_df['Date'])\n",
        "\n",
        "    if include_sentiment:\n",
        "        # Filter Twitter data for the specified stock\n",
        "        twitter_data = data[data['Stock Name'] == ticker].drop(columns=['Stock Name'])\n",
        "\n",
        "        # Group by day and compute mean sentiment scores\n",
        "        twitter_df = twitter_data.groupby('Day').mean()\n",
        "        twitter_df['Date'] = pd.to_datetime(twitter_df.index)\n",
        "        twitter_df = twitter_df.reset_index(drop=True)\n",
        "\n",
        "        # Merge Twitter and price data\n",
        "        df = pd.merge(twitter_df, price_df, on='Date', how='inner')\n",
        "    else:\n",
        "        # Use only price data\n",
        "        df = price_df.copy()\n",
        "\n",
        "    # Final clean-up\n",
        "    df = df.sort_values(by='Date').reset_index(drop=True)\n",
        "    df = df.drop(columns=['Date', 'Stock Name'], errors='ignore')\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7_KloPyDtGrr"
      },
      "outputs": [],
      "source": [
        "def download_stock_data(ticker, start_date, end_date):\n",
        "    \"\"\"\n",
        "    Download and clean historical stock data for a given ticker.\n",
        "    \"\"\"\n",
        "    data = yf.download(ticker, start=start_date, end=end_date)\n",
        "    data.reset_index(inplace=True)\n",
        "    data['Stock Name'] = ticker.upper()\n",
        "    data = data[['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Stock Name']]\n",
        "    return data\n",
        "\n",
        "def get_stock_data(tickers, start_date, end_date, temp_file=\"clean.csv\"):\n",
        "    \"\"\"\n",
        "    Downloads, saves, reads, and aggregates stock data for a list of tickers.\n",
        "    \"\"\"\n",
        "    final_df = pd.DataFrame()\n",
        "\n",
        "    for i, ticker in enumerate(tickers):\n",
        "        # Download and save to CSV\n",
        "        df = download_stock_data(ticker, start_date, end_date)\n",
        "        df.to_csv(temp_file, index=False)\n",
        "\n",
        "        # Read the saved CSV\n",
        "        df = pd.read_csv(temp_file)\n",
        "\n",
        "        # Skip the first row (for some reason this is part of your logic)\n",
        "        df = df.iloc[1:].reset_index(drop=True)\n",
        "\n",
        "        # Combine data\n",
        "        final_df = pd.concat([final_df, df], ignore_index=True)\n",
        "\n",
        "    # Convert data types\n",
        "    cols_to_convert = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "    final_df[cols_to_convert] = final_df[cols_to_convert].astype(float)\n",
        "\n",
        "    # Convert 'Date' column to datetime\n",
        "    final_df['Date'] = pd.to_datetime(final_df['Date'])\n",
        "\n",
        "    # Optional: Clean up the temp file if needed\n",
        "    if os.path.exists(temp_file):\n",
        "        os.remove(temp_file)\n",
        "\n",
        "    return final_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "m69JAfUqd48v"
      },
      "outputs": [],
      "source": [
        "def split_X_y(df, target_col, method='minmax', include_sent=True, n_input=5, n_output=1, add_indicators=True):\n",
        "    \"\"\"\n",
        "    Prepares time-series input and binary target sequences for model training and testing.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): Input DataFrame containing stock prices and optional sentiment columns.\n",
        "        target_col (str): Column name to use as the target for prediction (typically 'Close').\n",
        "        method (str): Scaling method to apply. Options are:\n",
        "                      - 'minmax' for MinMaxScaler\n",
        "                      - 'standard' for StandardScaler\n",
        "                      - None for no scaling\n",
        "        include_sent (bool): Whether to include sentiment columns ('negative', 'neutral', 'positive') in features.\n",
        "        n_input (int): Number of time steps in the input sequence.\n",
        "        n_output (int): Number of time steps ahead to define the target (used for binary classification).\n",
        "        add_indicators (bool): Whether to add technical indicators (SMA, RSI, MACD) to features.\n",
        "\n",
        "    Returns:\n",
        "        X_train (np.ndarray): Training feature sequences of shape (samples, n_input, features).\n",
        "        y_train (np.ndarray): Binary labels for training set (1 if future > current, else 0).\n",
        "        X_test (np.ndarray): Test feature sequences.\n",
        "        y_test (np.ndarray): Test labels.\n",
        "        scaler_x (sklearn Scaler or None): Scaler object used for feature transformation (useful for inverse transform).\n",
        "    \"\"\"\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    # Step 1: Optionally drop sentiment columns\n",
        "    if not include_sent:\n",
        "        for col in ['negative', 'neutral', 'positive']:\n",
        "            if col in df.columns:\n",
        "                df = df.drop(columns=[col])\n",
        "\n",
        "    # Step 2: Train-test split (raw, BEFORE any indicators)\n",
        "    num_in_train_raw = round(len(df) * 0.7)\n",
        "    df_train_raw = df.iloc[:num_in_train_raw].copy()\n",
        "    df_test_raw = df.iloc[num_in_train_raw - n_input - n_output + 1:].copy()\n",
        "\n",
        "    # Step 3: Add indicators SEPARATELY for train and test\n",
        "    if add_indicators:\n",
        "        for temp_df in [df_train_raw, df_test_raw]:\n",
        "            temp_df['SMA_10'] = ta.trend.sma_indicator(temp_df['Close'], window=10)\n",
        "            temp_df['SMA_20'] = ta.trend.sma_indicator(temp_df['Close'], window=20)\n",
        "            temp_df['RSI'] = ta.momentum.rsi(temp_df['Close'], window=14)\n",
        "\n",
        "            macd = ta.trend.MACD(temp_df['Close'])\n",
        "            temp_df['MACD'] = macd.macd()\n",
        "            temp_df['MACD_signal'] = macd.macd_signal()\n",
        "\n",
        "    # Step 4: Drop rows with NaN values (from indicators)\n",
        "    df_train_raw = df_train_raw.dropna().reset_index(drop=True)\n",
        "    df_test_raw = df_test_raw.dropna().reset_index(drop=True)\n",
        "\n",
        "    # Step 5: Select feature columns (exclude target)\n",
        "    feature_cols = df_train_raw.columns.tolist()\n",
        "\n",
        "    # Step 6: Scaling\n",
        "    scaler_x = None\n",
        "    if method is not None:\n",
        "        if method == 'minmax':\n",
        "            Scaler = MinMaxScaler\n",
        "        elif method == 'standard':\n",
        "            Scaler = StandardScaler\n",
        "        else:\n",
        "            raise ValueError(\"method must be 'minmax', 'standard', or None.\")\n",
        "\n",
        "        scaler_x = Scaler()\n",
        "        scaler_x.fit(df_train_raw[feature_cols])\n",
        "        df_train_raw[feature_cols] = scaler_x.transform(df_train_raw[feature_cols])\n",
        "        df_test_raw[feature_cols] = scaler_x.transform(df_test_raw[feature_cols])\n",
        "\n",
        "    # Step 7: Sequence creation\n",
        "    def make_sequences(data):\n",
        "        X_seq, y_seq = [], []\n",
        "        for i in range(len(data) - n_input - n_output + 1):\n",
        "            X_seq.append(data.iloc[i:i + n_input][feature_cols].values)\n",
        "            last_close = data.iloc[i + n_input - 1][target_col]\n",
        "            future_close = data.iloc[i + n_input + n_output - 1][target_col]\n",
        "            y_seq.append(1 if future_close > last_close else 0)\n",
        "        return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "    X_train, y_train = make_sequences(df_train_raw)\n",
        "    X_test, y_test = make_sequences(df_test_raw)\n",
        "\n",
        "    return X_train, y_train, X_test, y_test, scaler_x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLpI0GeI4E-W"
      },
      "source": [
        "#### Make the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PhSHkG8JAZ3-"
      },
      "outputs": [],
      "source": [
        "# Load the sentiment data from a CSV file\n",
        "data_sent = pd.read_csv('data/data_sentiment.csv')\n",
        "\n",
        "# Preprocessing\n",
        "data_sent['Date'] = pd.to_datetime(data_sent['Date'])\n",
        "data_sent['Day'] = data_sent['Date'].dt.date\n",
        "data_sent.drop(columns=['Date', 'Tweet', 'Company Name'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2krNsZBqyGKz"
      },
      "outputs": [],
      "source": [
        "# Load the historical prices data from a CSV file\n",
        "prices_hist = pd.read_csv('data/stock_yfinance_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b-tzsNV0iiI",
        "outputId": "68822689-885a-43e7-fd5f-8db9e97e7676"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-8-1149963138.py:5: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  data = yf.download(ticker, start=start_date, end=end_date)\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "/tmp/ipython-input-8-1149963138.py:5: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  data = yf.download(ticker, start=start_date, end=end_date)\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "/tmp/ipython-input-8-1149963138.py:5: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  data = yf.download(ticker, start=start_date, end=end_date)\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "/tmp/ipython-input-8-1149963138.py:5: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  data = yf.download(ticker, start=start_date, end=end_date)\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "/tmp/ipython-input-8-1149963138.py:5: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  data = yf.download(ticker, start=start_date, end=end_date)\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "/tmp/ipython-input-8-1149963138.py:5: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  data = yf.download(ticker, start=start_date, end=end_date)\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "/tmp/ipython-input-8-1149963138.py:5: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  data = yf.download(ticker, start=start_date, end=end_date)\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "/tmp/ipython-input-8-1149963138.py:5: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  data = yf.download(ticker, start=start_date, end=end_date)\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        }
      ],
      "source": [
        "tickers = ['TSLA', 'MSFT', 'PG', 'META', 'AMZN', 'GOOG', 'AMD', 'AAPL']\n",
        "final_df = get_stock_data(tickers, '2024-07-20', '2025-07-20')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzvOCNvIwwVX"
      },
      "source": [
        "## Trading Simulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "N2lraFg0wuv5"
      },
      "outputs": [],
      "source": [
        "def simulate_intraday_long_short(model, X_test_flat, price_data_open, price_data_close, initial_cash=10000):\n",
        "    \"\"\"\n",
        "    Simulate daily long/short strategy:\n",
        "    - Go long if prediction is class 1, close at end of day.\n",
        "    - Go short if prediction is class 0, cover at end of day.\n",
        "\n",
        "    Parameters:\n",
        "        model: Trained classifier with predict method\n",
        "        X_test_flat: Features for prediction\n",
        "        price_data_open: Opening prices aligned with X_test\n",
        "        price_data_close: Closing prices aligned with X_test\n",
        "        initial_cash: Starting capital\n",
        "\n",
        "    Returns:\n",
        "        final_cash: Final value after trading\n",
        "        trades: List of trade details\n",
        "    \"\"\"\n",
        "    preds = model.predict(X_test_flat)\n",
        "    cash = initial_cash\n",
        "    trades = []\n",
        "\n",
        "    for i in range(len(preds)):\n",
        "        open_price = price_data_open[i]\n",
        "        close_price = price_data_close[i]\n",
        "\n",
        "        if preds[i] == 1:\n",
        "            # Long trade: Buy at open, sell at close\n",
        "            shares = cash // open_price\n",
        "            profit = shares * (close_price - open_price)\n",
        "            cash += profit\n",
        "            trades.append({\n",
        "                'day': i,\n",
        "                'action': 'long',\n",
        "                'open_price': open_price,\n",
        "                'close_price': close_price,\n",
        "                'shares': shares,\n",
        "                'profit': profit\n",
        "            })\n",
        "\n",
        "        elif preds[i] == 0:\n",
        "            # Short trade: Sell borrowed shares at open, buy back at close\n",
        "            shares = cash // open_price\n",
        "            profit = shares * (open_price - close_price)\n",
        "            cash += profit\n",
        "            trades.append({\n",
        "                'day': i,\n",
        "                'action': 'short',\n",
        "                'open_price': open_price,\n",
        "                'close_price': close_price,\n",
        "                'shares': shares,\n",
        "                'profit': profit\n",
        "            })\n",
        "\n",
        "    return cash, trades"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSf0IgdhOecm"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3thOkOqlzHF"
      },
      "outputs": [],
      "source": [
        "def create_lstm_model(n_input, num_features, depth=3):\n",
        "    model = tf.keras.Sequential()\n",
        "    for i in range(depth):\n",
        "        return_sequences = i < depth - 1\n",
        "        units = max(128 // (2 ** i), 32)\n",
        "        if i == 0:\n",
        "            model.add(layers.LSTM(units, return_sequences=return_sequences, input_shape=(n_input, num_features)))\n",
        "        else:\n",
        "            model.add(layers.LSTM(units, return_sequences=return_sequences))\n",
        "        model.add(layers.BatchNormalization())\n",
        "        model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.Dense(32, activation='relu'))\n",
        "    model.add(layers.Dense(16, activation='relu'))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation=\"relu\"),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "\n",
        "class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n",
        "    def __init__(self, sequence_length, num_features, embed_dim):\n",
        "        super().__init__()\n",
        "        self.dense_proj = layers.Dense(embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=sequence_length, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x, training=None):\n",
        "        x_proj = self.dense_proj(x)\n",
        "        positions = tf.range(start=0, limit=tf.shape(x)[1], delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        return x_proj + positions\n",
        "\n",
        "\n",
        "def create_transformer_model(n_input, num_features, depth=1):\n",
        "    embed_dim = 64\n",
        "    num_heads = 2\n",
        "    ff_dim = 128\n",
        "\n",
        "    inputs = layers.Input(shape=(n_input, num_features))\n",
        "    x = TokenAndPositionEmbedding(n_input, num_features, embed_dim)(inputs)\n",
        "\n",
        "    for _ in range(depth):\n",
        "        x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Dense(32, activation='relu')(x)\n",
        "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = models.Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twlthI6_l2wk"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_all_models(tickers, scalers, sent_flags, models_to_run, n_inputs=[5], n_runs=5, technical_indicators=True, depths=[{'LSTM': 3, 'Transformer': 1}]):\n",
        "    results = []\n",
        "\n",
        "    model_creators = {\n",
        "        'LSTM': create_lstm_model,\n",
        "        'Transformer': create_transformer_model,\n",
        "        'XGBoost': None,  # Special case\n",
        "        'LinearRegression': None  # Special case\n",
        "    }\n",
        "\n",
        "\n",
        "    for scaler in scalers:\n",
        "      for ti in technical_indicators:\n",
        "        for n_input in n_inputs:\n",
        "            for depth in depths:\n",
        "              for ticker in tickers:\n",
        "                for sent in sent_flags:\n",
        "\n",
        "                  df = df_for_inp(data_sent, prices_hist, ticker, sent)\n",
        "                  X_train, y_train, X_test, y_test, _ = split_X_y(df, 'Close', scaler, include_sent=sent, n_input=n_input, add_indicators=ti)\n",
        "\n",
        "                  for model_name in models_to_run:\n",
        "                      for run in range(n_runs):\n",
        "                          print(f\"[{model_name}] {ticker} | Scaler: {scaler} | Sent: {sent} | Run {run + 1} | Technical Indicators {ti} | Lag window {n_input} | Depth {depth}\")\n",
        "\n",
        "                          train_loss, val_loss, y_pred_probs = None, None, None\n",
        "\n",
        "                          if model_name == 'XGBoost':\n",
        "                              X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
        "                              X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
        "                              model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', verbosity=0)\n",
        "                              model.fit(X_train_flat, y_train)\n",
        "                              y_pred = model.predict(X_test_flat)\n",
        "\n",
        "                          elif model_name == 'LinearRegression':\n",
        "                              X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
        "                              X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
        "                              model = LinearRegression()\n",
        "                              model.fit(X_train_flat, y_train)\n",
        "                              y_pred_probs = model.predict(X_test_flat)\n",
        "                              y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "                          else:\n",
        "                              model_creator = model_creators[model_name]\n",
        "                              depth_temp = depth.get(model_name, 1)  # Default depth is 1\n",
        "                              model = model_creator(X_train.shape[1], X_train.shape[2], depth=depth_temp)\n",
        "                              early_stop = EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=True)\n",
        "                              history = model.fit(X_train, y_train, epochs=500, batch_size=16, validation_split=0.1, callbacks=[early_stop], verbose=0)\n",
        "                              train_loss = history.history['loss']\n",
        "                              val_loss = history.history['val_loss']\n",
        "                              y_pred_probs = model.predict(X_test)\n",
        "                              y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
        "\n",
        "                          acc = accuracy_score(y_test, y_pred)\n",
        "                          f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "                          class_dist = dict(zip(*np.unique(y_pred, return_counts=True)))\n",
        "\n",
        "                          results.append({\n",
        "                              'model': model_name,\n",
        "                              'ticker': ticker,\n",
        "                              'scaler': scaler,\n",
        "                              'sentiment': sent,\n",
        "                              'run': run + 1,\n",
        "                              'accuracy': acc,\n",
        "                              'f1_score': f1,\n",
        "                              'train_loss': train_loss,\n",
        "                              'val_loss': val_loss,\n",
        "                              'class_distribution': class_dist,\n",
        "                              'lag': n_input,\n",
        "                              'pred_probs': y_pred_probs if y_pred_probs is not None else None,\n",
        "                              'technical_indicators': ti,\n",
        "                              'depth': depth_temp\n",
        "                          })\n",
        "\n",
        "                          pd.DataFrame(results).to_csv(\"training_results2.csv\", index=False)\n",
        "\n",
        "    return pd.DataFrame(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohNfkVpVmJGQ",
        "outputId": "5f8e0449-53ea-46f0-c190-06aeeec5282e"
      },
      "outputs": [],
      "source": [
        "tickers = ['TSLA', 'MSFT', 'PG', 'META', 'AMZN', 'GOOG', 'AMD', 'AAPL']\n",
        "scalers = ['standard']#, 'minmax'None,\n",
        "sent_flags = [True, False]\n",
        "models_to_run = ['LSTM', 'XGBoost', 'LinearRegression', 'Transformer']\n",
        "technical_indicators = [True, False]\n",
        "lag_windows = [5, 10, 15]\n",
        "depths = [{'LSTM': i, 'Transformer': i} for i in range(10,17,3)] + [{'LSTM': i, 'Transformer': i} for i in range(19,34,7)]\n",
        "\n",
        "results_df = train_and_evaluate_all_models(tickers, scalers, sent_flags, models_to_run, n_inputs=lag_windows, n_runs=3, technical_indicators=technical_indicators, depths=depths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZMfugGfENco"
      },
      "source": [
        "## Trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "YVzTe8A0EMzZ"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_models(tickers, scalers, sent_flags, models_to_run,\n",
        "                               n_inputs=[5], n_runs=5, technical_indicators=True,\n",
        "                               xgb_params=None, ada_params=None):\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Default hyperparameters if none provided\n",
        "    if xgb_params is None:\n",
        "        xgb_params = {\n",
        "            'max_depth': [3],\n",
        "            'min_child_weight': [1],\n",
        "            'gamma': [0],\n",
        "            'learning_rate': [0.1],\n",
        "            'n_estimators': [100]\n",
        "        }\n",
        "\n",
        "    if ada_params is None:\n",
        "        ada_params = {\n",
        "            'base_estimator__max_depth': [1],\n",
        "            'learning_rate': [1.0],\n",
        "            'n_estimators': [50]\n",
        "        }\n",
        "\n",
        "    for scaler in scalers:\n",
        "        for ti in technical_indicators:\n",
        "            for n_input in n_inputs:\n",
        "                for ticker in tickers:\n",
        "                    for sent in sent_flags:\n",
        "\n",
        "                        df = df_for_inp(data_sent, final_df, ticker, sent)\n",
        "                        X_train, y_train, X_test, y_test, _ = split_X_y(df, 'Close', scaler, include_sent=sent, n_input=n_input, add_indicators=ti)\n",
        "\n",
        "                        # Flatten inputs for classical ML\n",
        "                        X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
        "                        X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "                        for model_name in models_to_run:\n",
        "                            for run in range(n_runs):\n",
        "\n",
        "                                print(f\"[{model_name}] {ticker} | Scaler: {scaler} | Sent: {sent} | Run {run + 1} | TI: {ti} | Lag: {n_input}\")\n",
        "\n",
        "                                if model_name == 'XGBoost':\n",
        "                                    for max_depth in xgb_params['max_depth']:\n",
        "                                        for min_child_weight in xgb_params['min_child_weight']:\n",
        "                                            for gamma in xgb_params['gamma']:\n",
        "                                                for lr in xgb_params['learning_rate']:\n",
        "                                                    for n_estimators in xgb_params['n_estimators']:\n",
        "                                                        model = xgb.XGBClassifier(\n",
        "                                                            use_label_encoder=False,\n",
        "                                                            eval_metric='logloss',\n",
        "                                                            verbosity=0,\n",
        "                                                            max_depth=max_depth,\n",
        "                                                            min_child_weight=min_child_weight,\n",
        "                                                            gamma=gamma,\n",
        "                                                            learning_rate=lr,\n",
        "                                                            n_estimators=n_estimators\n",
        "                                                        )\n",
        "                                                        model.fit(X_train_flat, y_train)\n",
        "                                                        y_pred = model.predict(X_test_flat)\n",
        "\n",
        "                                                        acc = accuracy_score(y_test, y_pred)\n",
        "                                                        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "                                                        class_dist = dict(zip(*np.unique(y_pred, return_counts=True)))\n",
        "\n",
        "                                                                                                   # Prepare price series\n",
        "                                                        test_open_prices = df.iloc[-len(X_test):]['Open'].values\n",
        "                                                        test_close_prices = df.iloc[-len(X_test):]['Close'].values\n",
        "\n",
        "                                                        # Simulate long/short trading\n",
        "                                                        profit = portfolio_value, trade_log = simulate_intraday_long_short(\n",
        "                                                            model=model,\n",
        "                                                            X_test_flat=X_test_flat,\n",
        "                                                            price_data_open=test_open_prices,\n",
        "                                                            price_data_close=test_close_prices,\n",
        "                                                            initial_cash=10000\n",
        "                                                        )\n",
        "\n",
        "                                                        results.append({\n",
        "                                                            'model': 'XGBoost',\n",
        "                                                            'ticker': ticker,\n",
        "                                                            'scaler': scaler,\n",
        "                                                            'sentiment': sent,\n",
        "                                                            'run': run + 1,\n",
        "                                                            'accuracy': acc,\n",
        "                                                            'f1_score': f1,\n",
        "                                                            'train_loss': None,\n",
        "                                                            'val_loss': None,\n",
        "                                                            'class_distribution': class_dist,\n",
        "                                                            'lag': n_input,\n",
        "                                                            'pred_probs': None,\n",
        "                                                            'technical_indicators': ti,\n",
        "                                                            'xgb_max_depth': max_depth,\n",
        "                                                            'xgb_min_child_weight': min_child_weight,\n",
        "                                                            'xgb_gamma': gamma,\n",
        "                                                            'learning_rate': lr,\n",
        "                                                            'n_estimators': n_estimators,\n",
        "                                                            'profit': profit[0] - 10000\n",
        "                                                        })\n",
        "\n",
        "                                elif model_name == 'AdaBoost':\n",
        "                                    for max_depth in ada_params['estimator__max_depth']:\n",
        "                                        for lr in ada_params['learning_rate']:\n",
        "                                            for n_estimators in ada_params['n_estimators']:\n",
        "                                                base_estimator = DecisionTreeClassifier(max_depth=max_depth)\n",
        "                                                model = AdaBoostClassifier(\n",
        "                                                    estimator=base_estimator,\n",
        "                                                    learning_rate=lr,\n",
        "                                                    n_estimators=n_estimators\n",
        "                                                )\n",
        "                                                model.fit(X_train_flat, y_train)\n",
        "                                                y_pred = model.predict(X_test_flat)\n",
        "\n",
        "                                                acc = accuracy_score(y_test, y_pred)\n",
        "                                                f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "                                                class_dist = dict(zip(*np.unique(y_pred, return_counts=True)))\n",
        "\n",
        "                                                # Prepare price series\n",
        "                                                test_open_prices = df.iloc[-len(X_test):]['Open'].values\n",
        "                                                test_close_prices = df.iloc[-len(X_test):]['Close'].values\n",
        "\n",
        "                                                # Simulate long/short trading\n",
        "                                                profit = portfolio_value, trade_log = simulate_intraday_long_short(\n",
        "                                                    model=model,\n",
        "                                                    X_test_flat=X_test_flat,\n",
        "                                                    price_data_open=test_open_prices,\n",
        "                                                    price_data_close=test_close_prices,\n",
        "                                                    initial_cash=10000\n",
        "                                                )\n",
        "\n",
        "                                                results.append({\n",
        "                                                    'model': 'AdaBoost',\n",
        "                                                    'ticker': ticker,\n",
        "                                                    'scaler': scaler,\n",
        "                                                    'sentiment': sent,\n",
        "                                                    'run': run + 1,\n",
        "                                                    'accuracy': acc,\n",
        "                                                    'f1_score': f1,\n",
        "                                                    'train_loss': None,\n",
        "                                                    'val_loss': None,\n",
        "                                                    'class_distribution': class_dist,\n",
        "                                                    'lag': n_input,\n",
        "                                                    'pred_probs': None,\n",
        "                                                    'technical_indicators': ti,\n",
        "                                                    'base_estimator_max_depth': max_depth,\n",
        "                                                    'learning_rate': lr,\n",
        "                                                    'n_estimators': n_estimators,\n",
        "                                                    'profit': profit[0] - 10000\n",
        "                                                })\n",
        "\n",
        "\n",
        "                        pd.DataFrame(results).to_csv(\"training_results_updated1.csv\", index=False)\n",
        "\n",
        "    return pd.DataFrame(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5Kg_I0_EZWS",
        "outputId": "23504be9-0a66-4492-a8b3-c16566351dcf"
      },
      "outputs": [],
      "source": [
        "tickers = ['TSLA', 'MSFT', 'PG', 'META', 'AMZN', 'GOOG', 'AMD', 'AAPL']\n",
        "scalers = ['standard', 'minmax', None]\n",
        "sent_flags = [False]\n",
        "technical_indicators = [True, False]\n",
        "lag_windows = [5, 10, 15]\n",
        "\n",
        "models_to_run = ['XGBoost', 'AdaBoost']\n",
        "\n",
        "xgb_hyperparams = {\n",
        "    'max_depth': [3, 5],\n",
        "    'min_child_weight': [1, 3],\n",
        "    'gamma': [0, 0.2],\n",
        "    'learning_rate': [0.05, 0.1],\n",
        "    'n_estimators': [100, 150]\n",
        "}\n",
        "\n",
        "ada_hyperparams = {\n",
        "    'estimator__max_depth': [1, 3, 4, 5],\n",
        "    'learning_rate': [0.5, 1.0],\n",
        "    'n_estimators': [50, 100]\n",
        "}\n",
        "\n",
        "results_df = train_and_evaluate_models(\n",
        "    tickers=tickers,\n",
        "    scalers=scalers,\n",
        "    sent_flags=sent_flags,\n",
        "    models_to_run=models_to_run,\n",
        "    n_inputs=lag_windows,\n",
        "    n_runs=3,\n",
        "    technical_indicators=technical_indicators,\n",
        "    xgb_params=xgb_hyperparams,\n",
        "    ada_params=ada_hyperparams\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyGjPG_d7tQn",
        "outputId": "56574dfc-741a-4a4d-9da7-b63d78b5467a"
      },
      "outputs": [],
      "source": [
        "tickers = ['TSLA', 'MSFT', 'PG', 'META', 'AMZN', 'GOOG', 'AMD', 'AAPL']\n",
        "scalers = [None]\n",
        "sent_flags = [False]\n",
        "technical_indicators = [False]\n",
        "lag_windows = [10]\n",
        "\n",
        "models_to_run = ['AdaBoost']\n",
        "\n",
        "xgb_hyperparams = {\n",
        "    'max_depth': [3],\n",
        "    'min_child_weight': [1],\n",
        "    'gamma': [0.2],\n",
        "    'learning_rate': [0.10],\n",
        "    'n_estimators': [100]\n",
        "}\n",
        "\n",
        "ada_hyperparams = {\n",
        "    'estimator__max_depth': [5],\n",
        "    'learning_rate': [1],\n",
        "    'n_estimators': [50]\n",
        "}\n",
        "\n",
        "results_df = train_and_evaluate_models(\n",
        "    tickers=tickers,\n",
        "    scalers=scalers,\n",
        "    sent_flags=sent_flags,\n",
        "    models_to_run=models_to_run,\n",
        "    n_inputs=lag_windows,\n",
        "    n_runs=100,\n",
        "    technical_indicators=technical_indicators,\n",
        "    xgb_params=xgb_hyperparams,\n",
        "    ada_params=ada_hyperparams\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
